{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2030865",
   "metadata": {},
   "source": [
    "# Part 5: Blob Knowledge Source\n",
    "\n",
    "In Parts 1-4, you worked with pre-indexed data, SharePoint, and web sources. In Part 5, you'll upload documents from Azure Blob Storage and create knowledge sources that index them automatically. You'll also compare two indexing modes: **minimal** (basic content extraction) and **standard** (advanced content understanding with Azure AI Services).\n",
    "\n",
    "## Step 1: Load Environment Variables\n",
    "\n",
    "Run below cell to load the configuration for your Azure resources, choose the **.venv(3.11.9)** environment that is created for you.\n",
    "\n",
    "Notice the additional variables for blob storage, AI services, and embedding models, which are needed for document ingestion and vectorization. All these Azure resources are pre-configured in `.env` for you.\n",
    "\n",
    "> **‚ö†Ô∏è Troubleshooting**\n",
    ">\n",
    "> If code cells get stuck and keep spinning, select **Restart** from the notebook toolbar at the top. If the issue persists after a couple of tries, close VS Code completely and reopen it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a4c30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# Azure AI Search configuration\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"])\n",
    "\n",
    " # Knowledge base name\n",
    "knowledge_base_name = \"upload-blob-knowledge-base-minimal\"\n",
    "standard_knowledge_base_name = \"upload-blob-knowledge-base-standard\"\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "azure_openai_chatgpt_deployment = os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\", \"gpt-4.1\")\n",
    "azure_openai_chatgpt_model_name = os.getenv(\"AZURE_OPENAI_CHATGPT_MODEL_NAME\", \"gpt-4.1\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\", \"text-embedding-3-large\")\n",
    "\n",
    "# Blob configuration\n",
    "blob_connection_string = os.environ.get(\"BLOB_CONNECTION_STRING\")\n",
    "blob_resource_id = os.environ.get(\"BLOB_RESOURCE_ID\")\n",
    "blob_container_name = os.environ[\"BLOB_CONTAINER_NAME\"]\n",
    "ai_services_endpoint = os.environ[\"AI_SERVICES_ENDPOINT\"]\n",
    "ai_services_key = os.environ[\"AI_SERVICES_KEY\"]\n",
    "\n",
    "blob_path = \"../data/ai-search-data/blobdata/MSFT_cloud_architecture_zava.pdf\"\n",
    "\n",
    "print(\"Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89e26d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Starting cleanup of Part 5 resources...\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-minimal\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-minimal\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-standard\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-standard\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-minimal\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-minimal\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-standard\n",
      "‚ú® Cleanup complete. You can now continue to Step 2.\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-standard\n",
      "‚ú® Cleanup complete. You can now continue to Step 2.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# OPTIONAL: CLEANUP CELL\n",
    "# Run this cell if you want to restart Part 5 from scratch.\n",
    "# It deletes the Knowledge Bases and Knowledge Sources created in this lab.\n",
    "# -------------------------------------------------------------------------\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# Ensure client is ready (uses variables from Step 1)\n",
    "if \"endpoint\" in globals() and \"credential\" in globals():\n",
    "    cleanup_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "    \n",
    "    items_to_delete = [\n",
    "        (\"Knowledge Base\", cleanup_client.delete_knowledge_base, \"upload-blob-knowledge-base-minimal\"),\n",
    "        (\"Knowledge Base\", cleanup_client.delete_knowledge_base, \"upload-blob-knowledge-base-standard\"),\n",
    "        (\"Knowledge Source\", cleanup_client.delete_knowledge_source, \"upload-blob-knowledge-source-minimal\"),\n",
    "        (\"Knowledge Source\", cleanup_client.delete_knowledge_source, \"upload-blob-knowledge-source-standard\"),\n",
    "    ]\n",
    "\n",
    "    print(\"üßπ Starting cleanup of Part 5 resources...\")\n",
    "    for label, delete_func, name in items_to_delete:\n",
    "        try:\n",
    "            delete_func(name)\n",
    "            print(f\"   ‚úÖ Deleted {label}: {name}\")\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"   ‚ö†Ô∏è {label} already deleted: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error deleting {name}: {e}\")\n",
    "            \n",
    "    print(\"‚ú® Cleanup complete. You can now continue to Step 2.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: Please run Step 1 first to load environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a57dec",
   "metadata": {},
   "source": [
    "## Step 2: Upload Document to Blob Storage\n",
    "\n",
    "Before creating a knowledge source, you need to upload a document to your blob storage. The code below uploads a PDF called `MSFT_cloud_architecture_zava.pdf` which contains information about Zava's cloud architecture and how they classify data by sensitivity level.\n",
    "\n",
    "Once you create the blob knowledge source in the next step, it will automatically find this PDF in the storage and index it for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b5472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup sample data in documents using Azure AD auth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.exceptions import ClientAuthenticationError, HttpResponseError\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Require an account URL for Azure AD auth (no keys). Prefer BLOB_ACCOUNT_URL in .env.\n",
    "account_url = os.environ.get(\"BLOB_ACCOUNT_URL\")\n",
    "\n",
    "# Fallback: derive account URL from the blob connection string without using the key\n",
    "if not account_url:\n",
    "    conn = os.environ.get(\"BLOB_CONNECTION_STRING\") or globals().get(\"blob_connection_string\")\n",
    "    if conn:\n",
    "        account_name = None\n",
    "        for part in conn.split(\";\"):\n",
    "            if part.lower().startswith(\"accountname=\"):\n",
    "                account_name = part.split(\"=\", 1)[1]\n",
    "                break\n",
    "        if account_name:\n",
    "            account_url = f\"https://{account_name}.blob.core.windows.net\"\n",
    "\n",
    "if not account_url:\n",
    "    raise ValueError(\"Missing BLOB_ACCOUNT_URL. Set it in .env or rerun setup-environment.sh to populate it.\")\n",
    "\n",
    "# Use Azure AD (managed identity/VS Code signed-in user/service principal) instead of account keys\n",
    "credential = DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "\n",
    "# Ensure container exists (idempotent)\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except HttpResponseError as e:\n",
    "    if e.status_code != 409:\n",
    "        raise\n",
    "except ClientAuthenticationError as e:\n",
    "    raise RuntimeError(\"Authentication failed. Ensure your identity has 'Storage Blob Data Contributor' on the storage account.\") from e\n",
    "\n",
    "blob_name = os.path.basename(blob_path)\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Upload directly; avoid exists() to reduce permission needs\n",
    "try:\n",
    "    with open(blob_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "except ClientAuthenticationError as e:\n",
    "    raise RuntimeError(\"Upload failed. Confirm your identity has 'Storage Blob Data Contributor' on the storage account.\") from e\n",
    "except HttpResponseError as e:\n",
    "    if getattr(e, \"error_code\", \"\").lower() == \"authorizationpermissionmismatch\":\n",
    "        raise RuntimeError(\"Authorization failed (AuthorizationPermissionMismatch). Ensure your identity has 'Storage Blob Data Contributor' on the storage account.\") from e\n",
    "    raise\n",
    "\n",
    "print(f\"Setup sample data in {blob_container_name} using Azure AD auth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d4627",
   "metadata": {},
   "source": [
    "## Step 3: Create Blob Knowledge Source with Minimal Extraction\n",
    "\n",
    "An **AzureBlobKnowledgeSource** automatically indexes documents from blob storage. Unlike the sources you've used before, this one ingests and processes the documents for you.\n",
    "\n",
    "The code below creates a knowledge source with a `content_extraction_mode` of **minimal**. This mode chunks documents quickly without deep semantic understanding. An embedding model (`text-embedding-3-large`) is used to vectorize the chunks for vector search, but the chunking strategy itself is basic and fast.\n",
    "\n",
    ">Minimal indexing is ideal when you need speed and have straightforward documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250a8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge source 'upload-blob-knowledge-source-minimal' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureBlobKnowledgeSource,\n",
    "    AzureBlobKnowledgeSourceParameters,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    KnowledgeSourceAzureOpenAIVectorizer,\n",
    "    KnowledgeSourceContentExtractionMode,\n",
    "    KnowledgeSourceIngestionParameters,\n",
    "    SearchIndexerDataNoneIdentity\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "embedding_model = KnowledgeSourceAzureOpenAIVectorizer(\n",
    "    azure_open_ai_parameters=AzureOpenAIVectorizerParameters(\n",
    "        resource_url=azure_openai_endpoint,\n",
    "        api_key=azure_openai_key,\n",
    "        deployment_name=azure_openai_embedding_deployment,\n",
    "        model_name=azure_openai_embedding_model_name\n",
    "    )\n",
    ")\n",
    "\n",
    "if blob_resource_id:\n",
    "    blob_connection = f\"ResourceId={blob_resource_id}\"\n",
    "else:\n",
    "    blob_connection = blob_connection_string\n",
    "\n",
    "if not blob_connection:\n",
    "    raise ValueError(\"Missing blob connection info. Set BLOB_RESOURCE_ID or BLOB_CONNECTION_STRING via setup-environment.sh.\")\n",
    "\n",
    "ingestion_identity = SearchIndexerDataNoneIdentity()  # system-assigned identity for ingestion\n",
    "\n",
    "knowledge_source = AzureBlobKnowledgeSource(\n",
    "    name=\"upload-blob-knowledge-source-minimal\",\n",
    "    azure_blob_parameters=AzureBlobKnowledgeSourceParameters(\n",
    "        connection_string=blob_connection,\n",
    "        container_name=blob_container_name,\n",
    "        ingestion_parameters=KnowledgeSourceIngestionParameters(\n",
    "            identity=ingestion_identity,\n",
    "            embedding_model=embedding_model,\n",
    "            content_extraction_mode=KnowledgeSourceContentExtractionMode.MINIMAL\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "index_client.create_or_update_knowledge_source(knowledge_source=knowledge_source)\n",
    "print(f\"Knowledge source '{knowledge_source.name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3331023",
   "metadata": {},
   "source": [
    "## Step 4: Check Knowledge Source Status\n",
    "\n",
    "After creating a blob knowledge source, it needs time to process the documents. The code below checks whether indexing is complete, in progress, or failed.\n",
    "\n",
    "Once you see that `itemsUpdatesProcessed` is 1, that means the single document has been indexed successfully. Once indexing is complete, you can move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc47525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"synchronizationStatus\": \"active\",\n",
      "  \"synchronizationInterval\": \"1d\",\n",
      "  \"lastSynchronizationState\": {\n",
      "    \"startTime\": \"2025-12-07T15:43:23.549Z\",\n",
      "    \"endTime\": \"2025-12-07T15:43:29.335Z\",\n",
      "    \"itemsUpdatesProcessed\": 1,\n",
      "    \"itemsUpdatesFailed\": 0,\n",
      "    \"itemsSkipped\": 0\n",
      "  },\n",
      "  \"statistics\": {\n",
      "    \"totalSynchronization\": 1,\n",
      "    \"averageSynchronizationDuration\": \"PT5.7864929S\",\n",
      "    \"averageItemsProcessedPerSynchronization\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "status = index_client.get_knowledge_source_status(knowledge_source.name)\n",
    "\n",
    "print(json.dumps(status.serialize(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744c762",
   "metadata": {},
   "source": [
    "## Step 5: Create Knowledge Base\n",
    "\n",
    "Now that the blob knowledge source has indexed the document, you can create a knowledge base to query it. The code below creates a knowledge base that uses the blob knowledge source you created earlier.\n",
    "\n",
    "Notice that this knowledge base also set `retrieval_reasoning_effort` to \"low\". Currently, the lowest possible effort is \"minimal\" and highest possible is \"medium\". The \"low\" effort will still perform query decomposition, but it will not do iterative retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869eec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base 'upload-blob-knowledge-base-minimal' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import AzureOpenAIVectorizerParameters, KnowledgeBase, KnowledgeBaseAzureOpenAIModel, KnowledgeRetrievalLowReasoningEffort, KnowledgeRetrievalOutputMode, KnowledgeSourceReference\n",
    "\n",
    "aoai_params = AzureOpenAIVectorizerParameters(\n",
    "    resource_url=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    deployment_name=azure_openai_chatgpt_deployment,\n",
    "    model_name=azure_openai_chatgpt_model_name,\n",
    ")\n",
    "\n",
    "knowledge_base = KnowledgeBase(\n",
    "    name=knowledge_base_name,\n",
    "    models=[KnowledgeBaseAzureOpenAIModel(azure_open_ai_parameters=aoai_params)],\n",
    "    knowledge_sources=[\n",
    "        KnowledgeSourceReference(name=knowledge_source.name)\n",
    "    ],\n",
    "    output_mode=KnowledgeRetrievalOutputMode.ANSWER_SYNTHESIS,\n",
    "    retrieval_reasoning_effort=KnowledgeRetrievalLowReasoningEffort\n",
    ")\n",
    "\n",
    "index_client.create_or_update_knowledge_base(knowledge_base)\n",
    "print(f\"Knowledge base '{knowledge_base_name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c50f9",
   "metadata": {},
   "source": [
    "## Step 6: Use agentic retrieval to fetch results from Blob Knowledge Source\n",
    "\n",
    "The code below queries the PDF document about Zava's data sensitivity classification levels. This demonstrates how agentic retrieval works with blob knowledge sources.\n",
    "\n",
    "When you run this query, the knowledge base analyzes your question, decomposes it into focused subqueries, searches the blob-indexed content concurrently, uses semantic ranking to filter results, and synthesizes a grounded answer with citations pointing back to the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5775dd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Zava's data sensitivity classification consists of three levels:\n",
       "\n",
       "- Level 1: Low business value. Examples include normal business communications (such as email) and files for administrative, sales, and support workers.\n",
       "- Level 2: Medium business value. Examples include financial and legal information, as well as research and development data for new products.\n",
       "- Level 3: High business value. Examples include customer and partner personally identifiable information, product engineering specifications, and proprietary manufacturing techniques [ref_id:0][ref_id:2]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.knowledgebases import KnowledgeBaseRetrievalClient\n",
    "from azure.search.documents.knowledgebases.models import AzureBlobKnowledgeSourceParams, KnowledgeBaseMessage, KnowledgeBaseMessageTextContent, KnowledgeBaseRetrievalRequest\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "if \"endpoint\" not in globals() or \"knowledge_base_name\" not in globals():\n",
    "    raise RuntimeError(\"Missing notebook state. Rerun Steps 1-5 to reload endpoint, credential, and knowledge_base_name.\")\n",
    "\n",
    "# Prefer admin key if present; otherwise fall back to AAD (managed identity/service principal) for retrieval\n",
    "admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "search_credential = AzureKeyCredential(admin_key) if admin_key else DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "\n",
    "# If the knowledge source object is not in scope (e.g., after a kernel restart), refetch it by name\n",
    "if \"knowledge_source\" not in globals():\n",
    "    knowledge_source = index_client.get_knowledge_source(\"upload-blob-knowledge-source-minimal\")\n",
    "\n",
    "knowledge_base_client = KnowledgeBaseRetrievalClient(endpoint=endpoint, knowledge_base_name=knowledge_base_name, credential=search_credential)\n",
    "\n",
    "blob_ks_params = AzureBlobKnowledgeSourceParams(\n",
    "    knowledge_source_name=knowledge_source.name,\n",
    "    include_references=True,\n",
    "    include_reference_source_data=True\n",
    ")\n",
    "req = KnowledgeBaseRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeBaseMessage(role=\"user\", content=[KnowledgeBaseMessageTextContent(text=\"What are the levels of Zava data sensitivity classification?\")])\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        blob_ks_params\n",
    "    ],\n",
    "    include_activity=True\n",
    ")\n",
    "\n",
    "result = knowledge_base_client.retrieve(retrieval_request=req)\n",
    "display(Markdown(result.response[0].content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75661dee",
   "metadata": {},
   "source": [
    "## Step 7: Review Response, References, and Activity\n",
    "\n",
    "The two cells below show the citations and activity log from the blob knowledge source query.\n",
    "\n",
    "The references reveal which chunks from the PDF were used to answer your question. \n",
    "\n",
    "The activity log shows how the knowledge base processed your query and retrieved information from the blob-indexed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300eb48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"f397da4b4eb3_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_pages_17\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"No data sent across the Internet is in plain text form. Always use HTTPS connections, IPsec, or other end -to-end data \\n\\nencryption methods. \\n\\nEncryption for data at rest in \\n\\nthe cloud \\n\\n \\n\\nAll data stored on disks or elsewhere in the cloud must be in an encrypted form. \\n\\nACLs for least privilege \\n\\naccess \\n\\n \\n\\nAccount permissions to access resources in the cloud and what they are allowed to do must follow least-privilege guidelines. \\n\\n \\n\\nZava s data sensitivity classification \\nUsing the information in Microsoft s Data Classification Toolkit, Zava performed an analysis of their data and determined the following \\n\\nlevels. \\n \\n\\nLevel 1: Low business value Level 2: Medium business value Level 3: High business value \\n\\n \\n\\nData is encrypted and available only to \\n\\nauthenticated users \\n\\nProvided for all data stored on premises and in cloud- \\n\\nbased storage and workloads, such as Office 365. Data \\n\\nis encrypted while it resides in the service and in transit \\n\\nbetween the service and client devices. \\n\\nExamples of Level 1 data are normal business \\n\\ncommunications (email) and files for administrative, \\n\\nsales, and support workers. \\n\\n \\n\\nLevel 1 plus strong authentication and data \\n\\nloss protection \\n\\nStrong authentication includes multi-factor \\n\\nauthentication with SMS validation. Data loss \\n\\nprevention ensures that sensitive or critical \\n\\ninformation does not travel outside the on-premises \\n\\nnetwork. \\n\\nExamples of Level 2 data are financial and legal \\n\\ninformation and research and development data for \\n\\nnew products. \\n\\n \\n\\nLevel 2 plus the highest levels of encryption, \\n\\nauthentication, and auditing \\n\\nThe highest levels of encryption for data at rest and in \\n\\nthe cloud, compliant with regional regulations, \\n\\ncombined with multi-factor authentication with smart \\n\\ncards and granular auditing and alerting. \\n\\nExamples of Level 3 data are customer and partner \\n\\npersonally identifiable information and product \\n\\nengineering specifications and proprietary\"\n",
      "    },\n",
      "    \"reranker_score\": 3.3193343,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"f397da4b4eb3_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_pages_19\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"groups. \\n\\n2 Perform data classification analysis into \\n\\nthree levels \\n\\nZava performed a careful review and \\n\\ndetermined the three levels, which was used \\n\\nto determine the Microsoft cloud offering \\n\\nfeatures to protect Zava s most valuable data. \\n\\n3 Determine access, retention, and \\n\\ninformation protection policies for data \\n\\nlevels \\n\\nBased on the data levels, Zava determined \\n\\ndetailed requirements, which will be used to \\n\\nqualify future IT workloads being moved to \\n\\nthe cloud. \\n\\n \\n \\n\\nZava s information policies \\n \\n\\n \\nAccess Data retention Information protection \\n\\n \\n\\n \\n\\nLevel 1: Low \\n\\nbusiness value \\n\\n \\n\\n\\u2022 Allow access to all \\n \\n\\n6 months \\n \\n\\nUse encryption \\n\\n \\n\\n \\n\\nLevel 2: Medium \\n\\nbusiness value \\n\\n\\u2022 Allow access to Zava \\nemployees, subcontractors, and \\npartners \\n\\n\\u2022 Use MFA, TLS, and MAM \\n\\n \\n\\n \\n\\n2 years \\n\\n \\n\\n \\n\\nUse hash values for data integrity \\n\\n \\n\\n \\n\\nLevel 3: High \\n\\nbusiness value \\n\\n\\u2022 Allow access to executives and \\nleads in engineering and \\nmanufacturing \\n\\n\\u2022 RMS with managed network \\ndevices only \\n\\n \\n\\n \\n\\n7 years \\n\\n \\n\\nUse digital signatures for non- \\nrepudiation \\n\\n \\n\\nZava s path to cloud security readiness \\n \\n\\n \\n\\nZava s use of Office 365 security best practices \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nMicrosoft Cloud Security for \\n\\nEnterprise Architects \\n\\nhttp://aka.ms/cloudarchsecurity \\n\\nInformation Protection for \\n\\nOffice 365 \\n\\nhttp://aka.ms/o365infoprotect \\n\\nSecurity in a Cloud-Enabled World \\n\\nMicrosoft Virtual Academy Course \\n\\nhttp://aka.ms/securecustomermva \\n\\n \\n\\n \\n\\n \\n\\nSeptember 2017 \\u00a9 2016 Microsoft Corporation. All rights reserved. To send feedback about this documentation, please write to us at CloudAdopt@microsoft.com. \\n\\nData Loss Prevention (DLP) \\n\\nDLP policies for Exchange Online, SharePoint \\n\\nOnline, and OneDrive help prevent users from \\n\\naccidentally or intentionally sharing the data. \\n\\nMore information \\n\\nMulti-factor authentication (MFA) for \\n\\nimportant user accounts\"\n",
      "    },\n",
      "    \"reranker_score\": 2.7280796,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"2\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"f397da4b4eb3_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_pages_18\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"smart \\n\\ncards and granular auditing and alerting. \\n\\nExamples of Level 3 data are customer and partner \\n\\npersonally identifiable information and product \\n\\nengineering specifications and proprietary \\n\\nmanufacturing techniques. \\n\\nData classification toolkit \\n \\n\\nMapping Microsoft cloud offerings and features to Zava s data levels \\n \\n\\nSaaS \\n\\n \\n\\n \\n\\n\\u2022 HTTPS for all connections \\n\\u2022 Encryption at rest \\n\\nAzure PaaS Azure IaaS \\n\\n\\u2022 Support only HTTPS connections \\n\\u2022 Encrypt files stored in Azure \\n\\n\\u2022 Require HTTPS or IPsec for server \\naccess \\n\\n\\u2022 Azure disk encryption \\n\\n \\n\\n\\u2022 Azure AD multi-factor \\nauthentication (MFA) with SMS \\n\\n\\u2022 Use Azure Key Vault for \\nencryption keys \\n\\n\\u2022 Azure AD MFA with SMS \\n\\n \\n\\n\\u2022 MFA with SMS \\n\\n\\u2022 Azure Rights Management \\nSystem (RMS) \\n\\n\\u2022 Azure AD MFA with smart cards \\n\\u2022 Intune conditional access \\n\\n\\u2022 Azure RMS \\n\\u2022 Azure AD MFA with smart cards \\n\\n \\n\\n\\u2022 MFA with smart cards \\n\\n \\nContinued on next page \\n\\n7 6 5 4 3 2 This topic is 6 of 7 in a series 1 \\n\\nHow a fictional but representative global \\n\\norganization has implemented the \\n\\nMicrosoft Cloud \\n\\nZava in the \\n\\nMicrosoft \\n\\nCloud \\n\\nhttps://msdn.microsoft.com/library/hh204743.aspx\\n\\n\\nSecure email flow and mailbox audit \\n\\nlogging \\n\\nExchange Online Protection and Advanced \\n\\nThreat Protection (ATP) protect against \\n\\nunknown malware, viruses, and malicious \\n\\nURLs transmitted through emails. Mailbox \\n\\naudit logging helps determine who has \\n\\nlogged into user mailboxes, sent messages, \\n\\nand other activities performed by the mailbox \\n\\nowner, a delegated user, or an administrator. \\n\\nCloud security \\n\\nresources \\n\\n1 Optimize administrator accounts for the \\n\\ncloud \\n\\nZava did an extensive review of the existing \\n\\nWindows Server AD administrator accounts \\n\\nand set up a series of cloud administrator \\n\\naccounts and groups. \\n\\n2 Perform data classification analysis into \\n\\nthree levels \\n\\nZava performed a careful review and \\n\\ndetermined the three levels, which was used \\n\\nto determine the Microsoft cloud offering\"\n",
      "    },\n",
      "    \"reranker_score\": 2.4748113,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"3\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"f397da4b4eb3_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_pages_16\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"and groups used by all of Zava s \\nsubscriptions, with the exception of dev/test Azure \\nsubscriptions. \\n\\n \\n\\nhttps://www.microsoft.com/en-us/server-cloud/products/microsoft-intune/overview.aspx\\nhttps://www.microsoft.com/en-us/server-cloud/products/microsoft-intune/overview.aspx\\nhttps://products.office.com/en-us/business/get-latest-office-365-for-your-business-with-2016-visit-office\\nhttps://products.office.com/en-us/business/get-latest-office-365-for-your-business-with-2016-visit-office\\nhttps://products.office.com/en-us/business/get-latest-office-365-for-your-business-with-2016-visit-office\\nhttps://products.office.com/en-us/business/get-latest-office-365-for-your-business-with-2016-visit-office\\nhttps://www.microsoft.com/en-us/server-cloud/products/microsoft-intune/overview.aspx\\nhttps://www.microsoft.com/en-us/server-cloud/products/microsoft-intune/overview.aspx\\nhttps://products.office.com/en-us/business/get-latest-office-365-for-your-business-with-2016-visit-office\\nhttps://www.microsoft.com/dynamics/default.aspx\\nhttps://technet.microsoft.com/library/mt765146.aspx\\nmailto:cloudadopt@microsoft.com\\n\\n\\nLevel 1: Low \\n\\nbusiness value \\n\\nLevel 2: Medium \\n\\nbusiness value \\n\\nLevel 3: High \\n\\nbusiness value \\n\\n \\n\\nSecurity \\nZava is serious about their information security and protection. When transitioning their IT \\n\\ninfrastructure to a cloud-inclusive one, they made sure that their on-premises security requirements \\n\\nwere supported and implemented in Microsoft s cloud offerings. \\n\\n \\n\\nZava s security requirements in the cloud \\n \\n\\nStrong authentication to \\n\\ncloud resources \\n\\n \\n\\nCloud resource access must be authenticated and, where possible, leverage multi-factor authentication. \\n\\nEncryption for traffic across \\n\\nthe Internet \\n\\nNo data sent across the Internet is in plain text form. Always use HTTPS connections, IPsec, or other end -to-end data \\n\\nencryption methods. \\n\\nEncryption for data at rest in \\n\\nthe cloud\"\n",
      "    },\n",
      "    \"reranker_score\": 2.2401972,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"4\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"f397da4b4eb3_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_pages_28\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"Zava s business needs\\n\\n\\tNetworking\\n\\tZava s networking infrastructure\\n\\tOn-premises network\\n\\n\\tZava s app infrastructure\\n\\tInternet connectivity\\n\\tInternet presence\\n\\n\\tZava s network analysis\\n\\tZava s use of ExpressRoute\\n\\tZava s path to cloud networking readiness\\n\\n\\tIdentity\\n\\tZava s Windows Server AD forest\\n\\tZava s federated authentication infrastructure\\n\\tDirectory synchronization for Zava s Windows Server AD forest\\n\\tGeographical distribution of Zava authentication traffic\\n\\tAuthentication process example:\\n\\n\\tRedundancy for the headquarters authentication infrastructure in Azure IaaS\\n\\n\\tSubscriptions, licenses, and user accounts\\n\\tOrganization\\n\\tZava s structure\\n\\tSubscriptions\\n\\tLicenses\\n\\tUser accounts\\n\\tTenants:\\n\\n\\n\\tSecurity\\n\\tZava s security requirements in the cloud\\n\\tMapping Microsoft cloud offerings and features to Zava s data levels\\n\\n\\tEnterprise scenarios\\n\\tMoving historical transaction data to the cloud\\n\\tSecure SharePoint Online team sites for sensitive and highly\\n\\tSensitive protection   Highly confidential\"\n",
      "    },\n",
      "    \"reranker_score\": 2.196653,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "references = json.dumps([ref.as_dict() for ref in result.references], indent=2)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5fbb6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Log Steps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelQueryPlanning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>azureBlob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agenticReasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modelAnswerSynthesis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   type\n",
       "0    modelQueryPlanning\n",
       "1             azureBlob\n",
       "2      agenticReasoning\n",
       "3  modelAnswerSynthesis"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "activity_types = [{\"type\": a.type} for a in result.activity]\n",
    "\n",
    "df = pd.DataFrame(activity_types)\n",
    "\n",
    "print(\"Activity Log Steps\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca66d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Details\n",
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"modelQueryPlanning\",\n",
      "    \"elapsed_ms\": 1208,\n",
      "    \"input_tokens\": 1456,\n",
      "    \"output_tokens\": 51\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"elapsed_ms\": 601,\n",
      "    \"knowledge_source_name\": \"upload-blob-knowledge-source-minimal\",\n",
      "    \"query_time\": \"2025-12-07T18:23:00.459Z\",\n",
      "    \"count\": 5,\n",
      "    \"azure_blob_arguments\": {\n",
      "      \"search\": \"Zava data sensitivity classification levels\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"agenticReasoning\",\n",
      "    \"reasoning_tokens\": 11871,\n",
      "    \"retrieval_reasoning_effort\": {\n",
      "      \"kind\": \"low\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"type\": \"modelAnswerSynthesis\",\n",
      "    \"elapsed_ms\": 1409,\n",
      "    \"input_tokens\": 5398,\n",
      "    \"output_tokens\": 120\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "activity_content = json.dumps([a.as_dict() for a in result.activity], indent=2)\n",
    "print(\"Activity Details\")\n",
    "print(activity_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac380c8d",
   "metadata": {},
   "source": [
    "## Step 8: Use Standard extraction mode with Content Understanding\n",
    "\n",
    "In the previous steps, you created a blob knowledge source with minimal extraction mode. Now, you'll create another blob knowledge source using the **standard** extraction mode, which leverages Azure AI Services for deeper content understanding. This mode provides advanced chunking strategies, semantic extraction, and better handling of complex documents.\n",
    "\n",
    "The code below adds `content_extraction_mode=STANDARD` and connects Azure AI Services for enhanced processing. \n",
    "\n",
    ">Standard extraction takes longer but produces higher-quality chunks that preserve document structure and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22490f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ FORCE DELETE - Removing resources with wrong credentials...\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-standard\n",
      "   ‚úÖ Deleted Knowledge Base: upload-blob-knowledge-base-standard\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-standard\n",
      "   ‚è≥ Waiting 10 seconds for Azure to fully remove it...\n",
      "   ‚úÖ Deleted Knowledge Source: upload-blob-knowledge-source-standard\n",
      "   ‚è≥ Waiting 10 seconds for Azure to fully remove it...\n",
      "\n",
      "üîç Verifying deletion completed...\n",
      "\n",
      "üîç Verifying deletion completed...\n",
      "   ‚úÖ CONFIRMED: Knowledge Source is fully deleted!\n",
      "\n",
      "‚ñ∂Ô∏è  Creating BRAND NEW Knowledge Source with correct credentials...\n",
      "   Endpoint: https://lab511-ai-services-lgkxxgi4tkgcm.cognitiveservices.azure.com/\n",
      "   Key: 47bbe...1cf22 (expected: 1f37e...9ab3e)\n",
      "   ‚úÖ CONFIRMED: Knowledge Source is fully deleted!\n",
      "\n",
      "‚ñ∂Ô∏è  Creating BRAND NEW Knowledge Source with correct credentials...\n",
      "   Endpoint: https://lab511-ai-services-lgkxxgi4tkgcm.cognitiveservices.azure.com/\n",
      "   Key: 47bbe...1cf22 (expected: 1f37e...9ab3e)\n",
      "\n",
      "‚úÖ NEW Knowledge Source created!\n",
      "   üîë Credential check: 47bbe...1cf22\n",
      "\n",
      "‚è≥ Waiting 10 seconds for indexing to start...\n",
      "\n",
      "‚úÖ NEW Knowledge Source created!\n",
      "   üîë Credential check: 47bbe...1cf22\n",
      "\n",
      "‚è≥ Waiting 10 seconds for indexing to start...\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import AIServices, KnowledgeSourceContentExtractionMode\n",
    "from azure.core.exceptions import ResourceNotFoundError, HttpResponseError\n",
    "import time\n",
    "\n",
    "# CRITICAL: Azure Knowledge Sources store credentials PERMANENTLY\n",
    "# We MUST delete the old one completely before creating a new one\n",
    "ks_name = \"upload-blob-knowledge-source-standard\"\n",
    "kb_name = \"upload-blob-knowledge-base-standard\"\n",
    "\n",
    "print(\"üîÑ FORCE DELETE - Removing resources with wrong credentials...\")\n",
    "\n",
    "# Step 1: Delete Knowledge Base first (dependency)\n",
    "try:\n",
    "    index_client.delete_knowledge_base(kb_name)\n",
    "    print(f\"   ‚úÖ Deleted Knowledge Base: {kb_name}\")\n",
    "except ResourceNotFoundError:\n",
    "    print(f\"   ‚ÑπÔ∏è  Knowledge Base doesn't exist: {kb_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error deleting KB: {e}\")\n",
    "\n",
    "time.sleep(3)  # Wait for KB deletion to propagate\n",
    "\n",
    "# Step 2: Force delete Knowledge Source\n",
    "try:\n",
    "    # First try normal delete\n",
    "    index_client.delete_knowledge_source(ks_name)\n",
    "    print(f\"   ‚úÖ Deleted Knowledge Source: {ks_name}\")\n",
    "    print(\"   ‚è≥ Waiting 10 seconds for Azure to fully remove it...\")\n",
    "    time.sleep(10)  # Longer wait to ensure complete deletion\n",
    "except ResourceNotFoundError:\n",
    "    print(f\"   ‚ÑπÔ∏è  Knowledge Source doesn't exist: {ks_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error deleting KS: {e}\")\n",
    "\n",
    "# Step 3: VERIFY it's really gone (retry up to 5 times)\n",
    "print(\"\\nüîç Verifying deletion completed...\")\n",
    "ks_exists = True\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        index_client.get_knowledge_source(ks_name)\n",
    "        print(f\"   ‚ö†Ô∏è  Still exists (attempt {attempt+1}/5)... waiting 5 more seconds\")\n",
    "        time.sleep(5)\n",
    "        ks_exists = True\n",
    "    except ResourceNotFoundError:\n",
    "        print(f\"   ‚úÖ CONFIRMED: Knowledge Source is fully deleted!\")\n",
    "        ks_exists = False\n",
    "        break\n",
    "\n",
    "if ks_exists:\n",
    "    print(\"\\n‚ùå ERROR: Knowledge Source still exists after 30 seconds!\")\n",
    "    print(\"   Please wait 1 minute and run this cell again.\")\n",
    "    raise RuntimeError(\"Knowledge Source deletion did not complete\")\n",
    "\n",
    "# Step 4: Create completely NEW Knowledge Source\n",
    "print(\"\\n‚ñ∂Ô∏è  Creating BRAND NEW Knowledge Source with correct credentials...\")\n",
    "print(f\"   Endpoint: {ai_services_endpoint}\")\n",
    "print(f\"   Key: {ai_services_key[:5]}...{ai_services_key[-5:]} (expected: 1f37e...9ab3e)\")\n",
    "\n",
    "standard_knowledge_source = AzureBlobKnowledgeSource(\n",
    "    name=ks_name,\n",
    "    azure_blob_parameters=AzureBlobKnowledgeSourceParameters(\n",
    "        connection_string=blob_connection,\n",
    "        container_name=blob_container_name,\n",
    "        ingestion_parameters=KnowledgeSourceIngestionParameters(\n",
    "            identity=ingestion_identity,\n",
    "            embedding_model=embedding_model,\n",
    "            ai_services=AIServices(uri=ai_services_endpoint, api_key=ai_services_key),\n",
    "            content_extraction_mode=KnowledgeSourceContentExtractionMode.STANDARD\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use CREATE, not update\n",
    "try:\n",
    "    index_client.create_or_update_knowledge_source(knowledge_source=standard_knowledge_source)\n",
    "    print(f\"\\n‚úÖ NEW Knowledge Source created!\")\n",
    "    print(f\"   üîë Credential check: {ai_services_key[:5]}...{ai_services_key[-5:]}\")\n",
    "    print(f\"\\n‚è≥ Waiting 10 seconds for indexing to start...\")\n",
    "    time.sleep(10)\n",
    "except HttpResponseError as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"\\n‚ùå ERROR: Resource still exists! Wait 60 seconds and try again.\")\n",
    "        raise\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fe3c1",
   "metadata": {},
   "source": [
    "## Step 9: Check Standard Extraction Status\n",
    "\n",
    "Run below cell to monitor the standard extraction progress. This mode uses Azure AI Services to analyze document structure, recognize tables, and perform intelligent chunking, which takes more time than the minimal extraction mode we used earlier.\n",
    "\n",
    "Once you see that `itemsUpdatesProcessed` is 1, that means the single document has been indexed successfully. Once indexing is complete, you can move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aded544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"synchronizationStatus\": \"active\",\n",
      "  \"synchronizationInterval\": \"1d\",\n",
      "  \"lastSynchronizationState\": {\n",
      "    \"startTime\": \"2025-12-07T20:06:02.128Z\",\n",
      "    \"endTime\": \"2025-12-07T20:06:57.715Z\",\n",
      "    \"itemsUpdatesProcessed\": 1,\n",
      "    \"itemsUpdatesFailed\": 0,\n",
      "    \"itemsSkipped\": 0\n",
      "  },\n",
      "  \"statistics\": {\n",
      "    \"totalSynchronization\": 1,\n",
      "    \"averageSynchronizationDuration\": \"PT55.5871275S\",\n",
      "    \"averageItemsProcessedPerSynchronization\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "status = index_client.get_knowledge_source_status(standard_knowledge_source.name)\n",
    "\n",
    "print(json.dumps(status.serialize(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cf9c0",
   "metadata": {},
   "source": [
    "## Step 10: Create Knowledge Base for Standard Extraction\n",
    "\n",
    "You'll now create a knowledge base that uses the standard extraction blob knowledge source. This knowledge base will benefit from the enhanced document processing and improved chunk quality.\n",
    "\n",
    "Run below cell to create the knowledge base with the standard extraction source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f3f4931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base 'upload-blob-knowledge-base-standard' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import KnowledgeBase, KnowledgeBaseAzureOpenAIModel, KnowledgeRetrievalOutputMode, KnowledgeSourceReference\n",
    "\n",
    "standard_knowledge_base = KnowledgeBase(\n",
    "    name=standard_knowledge_base_name,\n",
    "    models=[KnowledgeBaseAzureOpenAIModel(azure_open_ai_parameters=aoai_params)],\n",
    "    knowledge_sources=[\n",
    "        KnowledgeSourceReference(name=standard_knowledge_source.name)\n",
    "    ],\n",
    "    output_mode=KnowledgeRetrievalOutputMode.ANSWER_SYNTHESIS\n",
    ")\n",
    "\n",
    "index_client.create_or_update_knowledge_base(standard_knowledge_base)\n",
    "print(f\"Knowledge base '{standard_knowledge_base_name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5d170",
   "metadata": {},
   "source": [
    "## Step 11: Query Standard Extraction Knowledge Base\n",
    "\n",
    "Run the same query about Zava's data sensitivity classification levels, but this time against the standard extraction knowledge base. \n",
    "\n",
    "Compare this response with the one from Step 6. You may notice differences in answer quality, completeness, or organization due to the improved document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "438e7eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Zava's data sensitivity classification consists of three levels:\n",
       "\n",
       "- Level 1: Low business value. This includes data that is encrypted and available only to authenticated users, such as normal business communications (email) and files for administrative, sales, and support workers. Data is encrypted both at rest and in transit, and is provided for all data stored on premises and in cloud-based storage and workloads [ref_id:0].\n",
       "\n",
       "- Level 2: Medium business value. This level adds strong authentication (such as multi-factor authentication with SMS validation) and data loss prevention to Level 1 protections. It covers financial and legal information and research and development data for new products. Data loss prevention ensures sensitive information does not leave the on-premises network [ref_id:0].\n",
       "\n",
       "- Level 3: High business value. This level includes the highest levels of encryption, authentication (multi-factor authentication with smart cards), and auditing, compliant with regional regulations. It applies to customer and partner personally identifiable information, product engineering specifications, and proprietary manufacturing techniques [ref_id:0]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.knowledgebases import KnowledgeBaseRetrievalClient\n",
    "from azure.search.documents.knowledgebases.models import AzureBlobKnowledgeSourceParams, KnowledgeBaseMessage, KnowledgeBaseMessageTextContent, KnowledgeBaseRetrievalRequest\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# FIX: Ensure we use the correct Search Endpoint (it might have been overwritten by Step 7)\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "\n",
    "if \"standard_knowledge_base_name\" not in globals():\n",
    "    raise RuntimeError(\"Missing notebook state. Rerun Steps 1-10 to reload standard knowledge base name.\")\n",
    "\n",
    "# Prefer admin key if present; otherwise fall back to AAD (managed identity/service principal) for retrieval\n",
    "admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "search_credential = AzureKeyCredential(admin_key) if admin_key else DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "\n",
    "# If the standard knowledge source object is not in scope (e.g., after a kernel restart), refetch it by name\n",
    "if \"standard_knowledge_source\" not in globals():\n",
    "    standard_knowledge_source = index_client.get_knowledge_source(\"upload-blob-knowledge-source-standard\")\n",
    "\n",
    "standard_knowledge_base_client = KnowledgeBaseRetrievalClient(endpoint=endpoint, knowledge_base_name=standard_knowledge_base_name, credential=search_credential)\n",
    "\n",
    "blob_ks_params = AzureBlobKnowledgeSourceParams(\n",
    "    knowledge_source_name=standard_knowledge_source.name,\n",
    "    include_references=True,\n",
    "    include_reference_source_data=True\n",
    ")\n",
    "req = KnowledgeBaseRetrievalRequest(\n",
    "    messages=[\n",
    "        KnowledgeBaseMessage(role=\"user\", content=[KnowledgeBaseMessageTextContent(text=\"What are the levels of Zava data sensitivity classification?\")])\n",
    "    ],\n",
    "    knowledge_source_params=[\n",
    "        blob_ks_params\n",
    "    ],\n",
    "    include_activity=True\n",
    ")\n",
    "\n",
    "result = standard_knowledge_base_client.retrieve(retrieval_request=req)\n",
    "display(Markdown(result.response[0].content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359f628",
   "metadata": {},
   "source": [
    "## Step 12: Compare Extraction Results\n",
    "\n",
    "The cell below shows citations from the standard extraction query.\n",
    "\n",
    "Compare these references with those from Step 7 to see how different extraction modes affect chunk creation and information retrieval from the same PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5569d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_15\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"<table>\\n<tr>\\n<th>Level 1: Low business value</th>\\n<th>Level 2: Medium business value</th>\\n<th>Level 3: High business value</th>\\n</tr>\\n<tr>\\n<td>Data is encrypted and available only to authenticated users</td>\\n<td>Level 1 plus strong authentication and data loss protection</td>\\n<td>Level 2 plus the highest levels of encryption, authentication, and auditing</td>\\n</tr>\\n<tr>\\n<td>Provided for all data stored on premises and in cloud- based storage and workloads, such as Office 365. Data is encrypted while it resides in the service and in transit between the service and client devices.</td>\\n<td>Strong authentication includes multi-factor authentication with SMS validation. Data loss prevention ensures that sensitive or critical information does not travel outside the on-premises network.</td>\\n<td>The highest levels of encryption for data at rest and in the cloud, compliant with regional regulations, combined with multi-factor authentication with smart cards and granular auditing and alerting.</td>\\n</tr>\\n<tr>\\n<td>Examples of Level 1 data are normal business communications (email) and files for administrative, sales, and support workers.</td>\\n<td>Examples of Level 2 data are financial and legal information and research and development data for new products.</td>\\n<td>Examples of Level 3 data are customer and partner personally identifiable information and product engineering specifications and proprietary manufacturing techniques.</td>\\n</tr>\\n</table>Data classification toolkitMapping Microsoft cloud offerings and features to Zava s data levels\"\n",
      "    },\n",
      "    \"reranker_score\": 2.9109857,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_16\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"<table>\\n<tr>\\n<th></th>\\n<th>SaaS</th>\\n<th>Azure PaaS</th>\\n<th>Azure IaaS</th>\\n</tr>\\n<tr>\\n<td>Level 1: Low business value</td>\\n<td>. HTTPS for all connections \\u00b7 Encryption at rest</td>\\n<td>. Support only HTTPS connections \\u00b7 Encrypt files stored in Azure</td>\\n<td>\\u00b7 Require HTTPS or IPsec for server access \\u00b7 Azure disk encryption</td>\\n</tr>\\n<tr>\\n<td>Level 2: Medium business value</td>\\n<td>\\u00b7 Azure AD multi-factor authentication (MFA) with SMS</td>\\n<td>\\u00b7 Use Azure Key Vault for encryption keys \\u00b7 Azure AD MFA with SMS</td>\\n<td>\\u00b7 MFA with SMS</td>\\n</tr>\\n<tr>\\n<td>Level 3: High business value</td>\\n<td>\\u00b7 Azure Rights Management System (RMS) \\u00b7 Azure AD MFA with smart cards . Intune conditional access</td>\\n<td>\\u00b7 Azure RMS \\u00b7 Azure AD MFA with smart cards</td>\\n<td>. MFA with smart cards</td>\\n</tr>\\n</table>Zava s information policies<table>\\n<tr>\\n<th></th>\\n<th>Access</th>\\n<th>Data retention</th>\\n<th>Information protection</th>\\n</tr>\\n<tr>\\n<td>Level 1: Low business value</td>\\n<td>. Allow access to all</td>\\n<td>6 months</td>\\n<td>Use encryption</td>\\n</tr>\\n<tr>\\n<td>Level 2: Medium business value</td>\\n<td>. Allow access to Zava employees, subcontractors, and partners \\u00b7 Use MFA, TLS, and MAM</td>\\n<td>2 years</td>\\n<td>Use hash values for data integrity</td>\\n</tr>\\n<tr>\\n<td>Level 3: High business value</td>\\n<td>. Allow access to executives and leads in engineering and manufacturing \\u00b7 RMS with managed network devices only</td>\\n<td>7 years</td>\\n<td>Use digital signatures for non- repudiation</td>\\n</tr>\\n</table>Zava s path to cloud security readiness1 Optimize administrator accounts for the cloudZava did an extensive review of the existing Windows Server AD administrator accounts and set up a series of cloud administrator accounts and groups.2 Perform data classification analysis into three levelsZava performed a careful review and determined the three levels, which was used to determine the Microsoft cloud offering features to protect Zava s most valuable data.3Determine access, retention, and \"\n",
      "    },\n",
      "    \"reranker_score\": 2.4696717,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"2\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_14\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"<table>\\n<tr>\\n<td>Strong authentication to cloud resources</td>\\n<td>Cloud resource access must be authenticated and, where possible, leverage multi-factor authentication.</td>\\n</tr>\\n<tr>\\n<td>Encryption for traffic across the Internet</td>\\n<td>No data sent across the Internet is in plain text form. Always use HTTPS connections, IPsec, or other end -to-end data encryption methods.</td>\\n</tr>\\n<tr>\\n<td>Encryption for data at rest in the cloud</td>\\n<td>All data stored on disks or elsewhere in the cloud must be in an encrypted form.</td>\\n</tr>\\n<tr>\\n<td>ACLs for least privilege access</td>\\n<td>Account permissions to access resources in the cloud and what they are allowed to do must follow least-privilege guidelines.</td>\\n</tr>\\n</table>Zava s data sensitivity classificationUsing the information in Microsoft s Data Classification Toolkit, Zava performed an analysis of their data and determined the following levels.\"\n",
      "    },\n",
      "    \"reranker_score\": 2.2781696,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"3\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_17\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \"va performed a careful review and determined the three levels, which was used to determine the Microsoft cloud offering features to protect Zava s most valuable data.3Determine access, retention, and information protection policies for data levelsBased on the data levels, Zava determined detailed requirements, which will be used to qualify future IT workloads being moved to the cloud.Zava s use of Office 365 security best practicesDedicated global administrator accountsThere are three, dedicated global administrator accounts with very strong passwords. Signing in with a global administrator account is only done for specific administrative tasks and the passwords are only known to designated staff.Multi-factor authentication (MFA) for important user accountsGlobal administrator accounts (to prevent credential compromise) and user accounts for managers (to prevent phishing attacks) have MFA enabled.More informationSecure email flow and mailbox audit loggingExchange Online Protection and Advanced Threat Protection (ATP) protect against unknown malware, viruses, and malicious URLs transmitted through emails. Mailbox audit logging helps determine who has logged into user mailboxes, sent messages, and other activities performed by the mailbox owner, a delegated user, or an administrator.Office 365 Email Anti-Spam ProtectionAdvanced threat protection for safe attachments and safe linksEnable mailbox auditing in Office 365Advanced Security Management (ASM)Policies for alerts so that IT administrators are notified of unusual or risky user activity, such as downloading large amounts of data, multiple failed sign-in attempts, or sign-ins from unknown or dangerous IP addressesMore informationData Loss Prevention (DLP)DLP policies for Exchange Online, SharePoint Online, and OneDrive help prevent users from accidentally or intentionally sharing the data.More informationCloud security resources<figure>\\n\\nMicrosoft Cloud Security for\\nEnterprise Architects\\n\\nhttp://aka.ms/cloudarchsec\"\n",
      "    },\n",
      "    \"reranker_score\": 2.125925,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"4\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_19\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \" an analysis of the tables in the databases that they intended to move to the cloud and fixed any issues. The new Stretch Database Advisor gave them a full overview of what they can expect from all features in SQL Server 2016, including which tables have cold data that could be stretched.2 UpgradeUpdated existing SQL servers in the Paris headquarters datacenter to SQL Server 2016.<figure>\\n\\nOn-premises\\nnetwork\\n\\nAzure PaaS\\n\\nLocal\\n\\nEligible data\\n\\nSQL\\n\\nSQL Server 2016\\n\\nSmart query\\nprocessing\\n\\nAzure SQL Stretch\\nDatabase\\n\\nT-SQL queries\\n\\n</figure>Stretch Database3 Migrate cold data to the cloudUsing SQL Management Studio, they identified the databases to stretch and the tables to migrate to instances of Stretch Database in Azure. Over time and in the background, SQL Server 2016 moved the historical data to stretch databases in Azure.Here is the resulting configuration for one server running SQL Server 2016 in the Paris headquarters:<figure>\\n\\nHeadquarters\\n\\nAzure PaaS\\n\\nUsers\\n\\nApp server\\n\\nSQL\\n\\nSQL Server 2016\\n\\nAzure SQL Stretch\\nDatabase\\n\\nDatacenter\\n\\nExpressRoute\\n\\n</figure>Users access the data through existing apps and queries. Access policies remain the same.Moving forward, there is no need for tape backups. Maintenance consists of backing up and restoring hot data.After implementing stretch database, Zava:\\u00b7 Reduced its on-premises data storage needs by 85%.\\u00b7 Made the update of the enterprise storage system and reliance on magnetic tape archives unnecessary.\\u00b7 Reduced its daily running costs significantly.Secure SharePoint Online team sites for sensitive and highly confidential assetsThe executive leadership of Zava want to use Office 365 and store their files in a single location for collaboration, regardless of where an executive might be. Similarly, Zava s research departments-with divisions in Paris, Moscow, New York, Beijing, and Bangalore-would like to transition their on-premises digital assets to the cloud for easier access and more open collaboration across teams.How\"\n",
      "    },\n",
      "    \"reranker_score\": 1.9393858,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"azureBlob\",\n",
      "    \"id\": \"5\",\n",
      "    \"activity_source\": 1,\n",
      "    \"source_data\": {\n",
      "      \"uid\": \"9cf272ed2627_aHR0cHM6Ly9sYWI1MTFzdGxna3h4Z2k0dGtnY20uYmxvYi5jb3JlLndpbmRvd3MubmV0L2RvY3VtZW50cy9NU0ZUX2Nsb3VkX2FyY2hpdGVjdHVyZV96YXZhLnBkZg2_text_sections_18\",\n",
      "      \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\",\n",
      "      \"snippet\": \" help prevent users from accidentally or intentionally sharing the data.More informationCloud security resources<figure>\\n\\nMicrosoft Cloud Security for\\nEnterprise Architects\\n\\nhttp://aka.ms/cloudarchsecurity\\n\\n</figure><figure>\\n\\n1897\\n\\n</figure>Information Protection for Office 365http://aka.ms/o365infoprotectSecurity in a Cloud-Enabled World Microsoft Virtual Academy Course http://aka.ms/securecustomermva<figure>\\n\\nMicrosoft\\n\\n</figure>Zava in the Microsoft CloudHow a fictional but representative global organization has implemented the Microsoft CloudThis topic is 7 of 7 in a series 1234567Enterprise scenariosWith the networking, identity, and security infrastructure in place, Zava began to address its business needs with enterprise cloud scenarios.Moving historical transaction data to the cloudZava's enterprise storage system stores a large amount of historical transaction data for adherence with regulatory requirements and for marketing research and BI analysis of spending trends. Zava also needs to restore archived data from magnetic tape, a time-intensive process. The hardware in Zava's enterprise storage system was nearing its end of life and replacing it would be very expensive.As part of its business need to scale down its on-premises datacenters, Zava chose to upgrade to SQL Server 2016 because of the Stretch Database hybrid feature and its seamless integration with Azure. Stretch Database allows Zava to move the cold data in its tables from on- premises to cloud storage, freeing up local disk space and reducing maintenance. Both hot and cold data are in the same tables and are always available to applications and their users and for maintenance, such as backups and restores.Zava used these steps to move their historical data to the cloud:1 Analyze databasesPerformed an analysis of the tables in the databases that they intended to move to the cloud and fixed any issues. The new Stretch Database Advisor gave them a full overview of what they can expect from all fe\"\n",
      "    },\n",
      "    \"reranker_score\": 1.9149274,\n",
      "    \"blob_url\": \"https://lab511stlgkxxgi4tkgcm.blob.core.windows.net/documents/MSFT_cloud_architecture_zava.pdf\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "references = json.dumps([ref.as_dict() for ref in result.references], indent=2)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8f19e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now experienced blob knowledge sources and compared different content extraction modes for document processing.\n",
    "\n",
    "**Key concepts to remember:**\n",
    "- `AzureBlobKnowledgeSource` automatically indexes documents from Azure Blob Storage\n",
    "- **Minimal extraction** (Steps 3-7): Fast, basic text extraction suitable for simple documents\n",
    "- **Standard extraction** (Steps 8-12): Uses Azure AI Services for advanced document understanding and better chunk quality\n",
    "- Standard extraction is beneficial for complex documents with tables, images, or intricate layouts\n",
    "- Both modes create searchable, vectorized chunks from your blob documents\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "‚û°Ô∏è Continue to [Part 6: Combined Knowledge Sources](part6-combined-knowledge-source.ipynb) to learn how to query search indexes, web URLs, SharePoint, and blob storage simultaneously in a single knowledge base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
