{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08eb9345",
   "metadata": {},
   "source": [
    "# Index Restoration Utility\n",
    "\n",
    "This notebook restores the pre-configured Azure AI Search indexes (`hrdocs` and `healthdocs`) used throughout the LAB511 workshop. The indexes contain pre-processed documents with embeddings and semantic configurations, so you can focus on learning Knowledge Bases APIs rather than data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "\n",
    "\n",
    "async def restore_index(endpoint: str, index_name: str, index_file: str, records_file: str, azure_openai_endpoint: str):\n",
    "    default_path = r\"../data/index-data\"\n",
    "    log_message = print\n",
    "    try:\n",
    "        log_message(f\"[{index_name}] Starting index restoration...\")\n",
    "        \n",
    "        # Create or update index\n",
    "        credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"])\n",
    "        async with SearchIndexClient(endpoint=endpoint, credential=credential) as client:\n",
    "            index_file_path = os.path.join(default_path, index_file)\n",
    "            log_message(f\"[{index_name}] Reading index definition from: {index_file_path}\")\n",
    "            \n",
    "            with open(index_file_path, \"r\", encoding=\"utf-8\") as in_file:\n",
    "                index_data = json.load(in_file)\n",
    "                index = SearchIndex.deserialize(index_data)\n",
    "                index.name = index_name\n",
    "                index.vector_search.vectorizers[0].parameters.resource_url = azure_openai_endpoint\n",
    "                index.vector_search.vectorizers[0].parameters.api_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "                log_message(f\"[{index_name}] Creating/updating index in Azure AI Search...\")\n",
    "                await client.create_or_update_index(index)\n",
    "                log_message(f\"[{index_name}] Index created/updated successfully\")\n",
    "\n",
    "        # Upload documents\n",
    "        async with SearchClient(endpoint=endpoint, index_name=index_name, credential=credential) as client:\n",
    "            records_file_path = os.path.join(default_path, records_file)\n",
    "            log_message(f\"[{index_name}] Reading documents from: {records_file_path}\")\n",
    "            \n",
    "            records = []\n",
    "            total_uploaded = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            with open(records_file_path, \"r\", encoding=\"utf-8\") as in_file:\n",
    "                for line_num, line in enumerate(in_file, 1):\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "                        records.append(record)\n",
    "                        \n",
    "                        if len(records) >= 100:\n",
    "                            batch_count += 1\n",
    "                            log_message(f\"[{index_name}] Uploading batch #{batch_count} ({len(records)} documents)...\")\n",
    "                            await client.upload_documents(documents=records)\n",
    "                            total_uploaded += len(records)\n",
    "                            records = []\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        log_message(f\"[{index_name}] WARNING: Skipping invalid JSON on line {line_num}: {e}\")\n",
    "                        continue\n",
    "\n",
    "            # Upload remaining documents\n",
    "            if records:\n",
    "                batch_count += 1\n",
    "                log_message(f\"[{index_name}] Uploading final batch #{batch_count} ({len(records)} documents)...\")\n",
    "                await client.upload_documents(documents=records)\n",
    "                total_uploaded += len(records)\n",
    "        \n",
    "        log_message(f\"[{index_name}] SUCCESS - Index restored! Total documents uploaded: {total_uploaded}\")\n",
    "        print(f\"Index {index_name} restored using {index_file} and {records_file}\")\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        error_msg = f\"[{index_name}] ERROR - File not found: {e}\"\n",
    "        log_message(error_msg)\n",
    "        log_message(f\"[{index_name}] Traceback:\\n{traceback.format_exc()}\")\n",
    "        print(f\"Index {index_name} failed - see log file for details\")\n",
    "        return False\n",
    "    except PermissionError as e:\n",
    "        error_msg = f\"[{index_name}] ERROR - Permission denied: {e}\"\n",
    "        log_message(error_msg)\n",
    "        log_message(f\"[{index_name}] This indicates insufficient permissions for the service principal\")\n",
    "        log_message(f\"[{index_name}] Traceback:\\n{traceback.format_exc()}\")\n",
    "        print(f\"Index {index_name} failed - see log file for details\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[{index_name}] ERROR - {type(e).__name__}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        log_message(f\"[{index_name}] Traceback:\\n{traceback.format_exc()}\")\n",
    "        print(f\"Index {index_name} failed - see log file for details\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "azure_openai_chatgpt_deployment = os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\", \"gpt-4.1\")\n",
    "azure_openai_chatgpt_model_name = os.getenv(\"AZURE_OPENAI_CHATGPT_MODEL_NAME\", \"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore hrdocs index\n",
    "print(\"\\n--- Processing hrdocs index ---\")\n",
    "await restore_index(\n",
    "    endpoint, \n",
    "    \"hrdocs\", \n",
    "    \"index.json\", \n",
    "    \"hrdocs-exported.jsonl\", \n",
    "    azure_openai_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore healthdocs index\n",
    "print(\"\\n--- Processing healthdocs index ---\")\n",
    "await restore_index(\n",
    "    endpoint, \n",
    "    \"healthdocs\", \n",
    "    \"index.json\", \n",
    "    \"healthdocs-exported.jsonl\", \n",
    "    azure_openai_endpoint \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5863ce",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once both indexes are restored successfully, you can proceed with the LAB511 workshop notebooks.\n",
    "\n",
    "➡️ Start with [Part 1: Single Knowledge Source - HR Docs](part1-single-knowledge-source-hr-docs.ipynb) to begin your journey into building advanced knowledge bases with Azure AI Search!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
